(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{271:function(n,t,$){"use strict";$.r(t);var o=$(77),e=$.n(o),_={data:function(){return{link:"content2",prekiji:"\n## 天才たちの築いた分野「情報理論」\n\n　ウィーナとシャノンが確立したと言われる分野である。\n通信に関して根深いイメージを持たれそうなものであるが、\nその実は数学的学問そのものである。\n\n　あらゆる分野において共通した領域があり、その領域に関して、\nプログレッシブな観点で、差異が生じることを嘆かわしく思い、\nウィーナはこれがモチベーションであったのではないかと言われている。\n\n　基本コンセプトはシャノンによって提唱されたものを基に確立されている。\nシャノンの通信モデルと言われるものである。\n汎用的な概念を抽象化し、モデル化して定式化した。\nコンピュータをやる人間にとっては尊敬の対象である。\n\n\n## 情報量とエントロピー\n\n　情報工学専攻の学生ならば、絶対に知っているものであると思うが、\n一応述べておくと、情報量自体はエントロピーではない。\n\n### Definition. (情報量)\n\n---\n\n　確率事象Eが生起したことを知ったとき,\n\n`$$I(E)=-\\log_2P(E)　[bit]$$`\n\nの __情報量__ (amount of information) を受け取ったという.\n\n---\n\n　例えばサイコロの目が偶数であるという事象 `$E_1$` は\n\n`$$P(E_1)=\\frac{1}{2}$$`\n\nよって\n\n`$$I(E_1)=-\\log_2\\frac{1}{2} = 1　[bit]$$`\n\nこれから考えると、確率が増える毎に情報量は減少することがわかる。\n\n### Definition. (エントロピー)\n\n---\n\n　離散的情報源 `$S$` で,出力シンボルを一つ知らされる毎に,受け取る情報量の期待値を,\nその情報源の __シンボルあたりの平均情報量__ (average amount of information per symbol) あるいは __エントロピー__ (entropy) とよび,\nこれを `$H(S)$` で表す.\n　特に記憶のない離散的情報源では,\n\n`$$H(S)=\\sum_{i=0}^{r}P(s_i)\\log_2\\frac{1}{P(s_i)}　[bit / symbol]$$`\n\nである.\n\n---\n\n　エントロピーとは熱力学の言葉で、力学系の有する無秩序さの度合いを表す量である。\n統計力学的見地では、断熱的可逆変化においては系全体のエントロピーは不変に保たれる。\n断熱的不可逆変化では、系全体のエントロピーは絶対的に増大する。聞いたことあるでしょう。\n\n\n## なんでlogなんだろう\n\n　理系ならばここで疑問に思わなくてはいけないことがある。\n何でlogを用いて情報量を定義しているのか。\nまず情報量を定義したいとなったときに、どういう感じになると嬉しいか考えると自然に理解できる。\nある関数を採用する上で、以下の性質を満たさないといけない。\n\n- 単調減少連続性\n- 加法性を満たす\n\nその上で「log、あなたでなければ私生きていけないの」と言わせなければならない。\n\n---\n\n### Theorem.\n\n　`$I$` を `$P$` のみの関数 `$I(P)$` とするとき,\n\n- (i) `$I(P)$` は `$P$` の単調減少連続関数\n- (ii) `$P_3=P_1・P_2$` とするとき,\n\n`$$I(P_3)=I(P_1)+I(P_2)$$`\n\nなる2つの条件を満たす関数は\n`$$I(P)=-k\\log_2P$$`\nに限られる.ここで `$k$` は正の定数である.\n\n---\n\n証明は今度書く。\n\n## あとがき\n\n　KaTeXをNuxtに組み込んでみたが、フォントがキモいのでどうにかしたいなあ。\n\n## 参考文献\n\n　非常に良書。[情報理論 (電子通信大学講座 第 39巻) (日本語) 単行本 – 1979/12/1 宮川 洋 (著)](https://www.amazon.co.jp/%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96-%E9%9B%BB%E5%AD%90%E9%80%9A%E4%BF%A1%E5%A4%A7%E5%AD%A6%E8%AC%9B%E5%BA%A7-%E7%AC%AC-39%E5%B7%BB-%E5%AE%AE%E5%B7%9D/dp/4339001023)\nでもちょっと誤植あるので注意。\n\n"}},computed:{kiji:function(){return e()(this.prekiji)}},components:{DefaultArticle:function(){return $.e(0).then($.bind(null,283))}}},r=$(20),component=Object(r.a)(_,(function(){var n=this.$createElement,t=this._self._c||n;return t("div",[t("DefaultArticle",{attrs:{content:{link:this.link,kiji:this.kiji}}})],1)}),[],!1,null,null,null);t.default=component.exports}}]);